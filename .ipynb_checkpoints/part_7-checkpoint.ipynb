{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, random, sys, io, re, string\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakes_lines = []\n",
    "poem_starts = []\n",
    "next_ln = False\n",
    "min_ = 100\n",
    "\n",
    "with open(\"data/shakespeare.txt\") as f:\n",
    "    \n",
    "    # Read in all lines\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        \n",
    "        # replace poem breaks with ~\n",
    "        if re.match('\\s+\\d+', line):\n",
    "            shakes_lines.append('\\n')\n",
    "            next_ln = True\n",
    "            continue\n",
    "            \n",
    "        # get rid of blank lines\n",
    "        seq = line.strip()\n",
    "        if len(seq) < 3:\n",
    "            continue\n",
    "        else:\n",
    "            min_ = len(seq)\n",
    "        # remove punctuation\n",
    "        seq = seq.translate(str.maketrans('', '', string.punctuation))\n",
    "        # make lowercase\n",
    "        seq = seq.lower()\n",
    "        #print(seq)\n",
    "        shakes_lines.append(seq)\n",
    "        \n",
    "        if next_ln:\n",
    "            poem_starts.append(seq)\n",
    "            next_ln = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spens_lines = []\n",
    "pstarts = []\n",
    "\n",
    "with open(\"data/spenser.txt\") as f:\n",
    "    \n",
    "    # Read in all lines\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        \n",
    "        # replace poem breaks with ~\n",
    "        if re.match('(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\n', line):\n",
    "            spens_lines.append('\\n')\n",
    "            next_ln = True\n",
    "            continue\n",
    "            \n",
    "        # get rid of blank lines\n",
    "        seq = line.strip()\n",
    "        if len(seq) < 3:\n",
    "            continue\n",
    "        else:\n",
    "            min_ = len(seq)\n",
    "        # remove punctuation\n",
    "        seq = seq.translate(str.maketrans('', '', string.punctuation))\n",
    "        # make lowercase\n",
    "        seq = seq.lower()\n",
    "        #print(seq)\n",
    "        spens_lines.append(seq)\n",
    "        \n",
    "        if next_ln:\n",
    "            pstarts.append(seq)\n",
    "            next_ln = False\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max line length:  58\n",
      "min line length:  46\n"
     ]
    }
   ],
   "source": [
    "processed_text = ' @ '.join(shakes_lines).replace(' @\\n @', '\\n')\\\n",
    "                 + ' @ '.join(spens_lines[:-1]).replace(' @\\n @\\n @\\n @', '\\n')\n",
    "# print(processed_text[:60*20])\n",
    "# print(poem_starts[:5])\n",
    "\n",
    "poems = processed_text.split('~')\n",
    "maxlen = max([len(ln) for ln in shakes_lines]) + 1\n",
    "print('max line length: ', maxlen)\n",
    "print('min line length: ', min_)\n",
    "#print(poems[:2])\n",
    "\n",
    "#print('total words', len(set(processed_text.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/processed_shakespeare.txt'\n",
    "vocab_size = 1000\n",
    "\n",
    "with open(fn, 'w') as f:\n",
    "    f.write(processed_text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bit kind of abuses SentencePiece's representation system a bit - it is designed to handle \"one sentence per line\" in the input, so I've modified the prepreprocessing to regard a poem as a \"sentence\" (i.e. one poem per line of input) with the line breaks in the poem converted to the @ symbol, which is made a user-defined symbol in the SentencePiece training so it will never become part of the representation of any other subword. We can convert back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(f\"--input={fn} --model_prefix=m --user_defined_symbols='<n>','@' --vocab_size={vocab_size} --model_type=bpe --pad_id=3\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "#sp.encode_as_pieces(poems[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "ok\n",
      "985\n",
      "7\n",
      "0\n",
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(209))\n",
    "print(sp.piece_to_id('@'))\n",
    "print(sp.piece_to_id('▁@'))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id), sp.is_control(id))\n",
    "\n",
    "sp.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase @ that thereby beautys rose might never die @ but as the riper should by time decease @ his tender heir might bear his memory @ but thou contracted to thine own bright eyes @ feedst thy lights flame with selfsubstantial fuel @ making a famine where abundance lies @ thy self thy foe to thy sweet self too cruel @ thou that art now the worlds fresh ornament @ and only herald to the gaudy spring @ within thine own bud buriest thy content @ and tender churl makst waste in niggarding @ pity the world or else this glutton be @ to eat the worlds due by the grave and thee @ \n",
      "['▁from', '▁fairest', '▁cre', 'ature', 's', '▁we', '▁desire', '▁inc', 'rea', 'se', '▁@', '▁that', '▁there', 'b', 'y', '▁beautys', '▁ro', 'se', '▁might', '▁never', '▁die', '▁@', '▁but', '▁as', '▁the', '▁ri', 'p', 'er', '▁should', '▁by', '▁time', '▁dec', 'ease', '▁@', '▁his', '▁tend', 'er', '▁he', 'ir', '▁might', '▁bear', '▁his', '▁mem', 'ory', '▁@', '▁but', '▁thou', '▁cont', 'ra', 'ct', 'ed', '▁to', '▁thine', '▁own', '▁bright', '▁eyes', '▁@', '▁fe', 'ed', 'st', '▁thy', '▁light', 's', '▁fl', 'ame', '▁with', '▁self', 'su', 'b', 'st', 'ant', 'ial', '▁f', 'u', 'el', '▁@', '▁making', '▁a', '▁fa', 'm', 'ine', '▁where', '▁ab', 'u', 'nd', 'ance', '▁lies', '▁@', '▁thy', '▁self', '▁thy', '▁fo', 'e', '▁to', '▁thy', '▁sweet', '▁self', '▁too', '▁cruel', '▁@', '▁thou', '▁that', '▁art', '▁now', '▁the', '▁worlds', '▁fresh', '▁or', 'na', 'ment', '▁@', '▁and', '▁only', '▁her', 'a', 'ld', '▁to', '▁the', '▁ga', 'u', 'dy', '▁spring', '▁@', '▁within', '▁thine', '▁own', '▁bu', 'd', '▁bu', 'ri', 'est', '▁thy', '▁cont', 'ent', '▁@', '▁and', '▁tend', 'er', '▁ch', 'ur', 'l', '▁mak', 'st', '▁w', 'aste', '▁in', '▁n', 'ig', 'g', 'ard', 'ing', '▁@', '▁pity', '▁the', '▁world', '▁or', '▁else', '▁this', '▁gl', 'ut', 't', 'on', '▁be', '▁@', '▁to', '▁ea', 't', '▁the', '▁worlds', '▁due', '▁by', '▁the', '▁gra', 've', '▁and', '▁thee', '▁@']\n",
      "len of data is 44408\n",
      "Encoding...\n",
      "(44388, 20) (44388,)\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of window_size Sentence Pieces\n",
    "spec = np.array(processed_text.split('\\n'))\n",
    "\n",
    "print(spec[0])\n",
    "print(sp.encode_as_pieces(spec[0]))\n",
    "\n",
    "spec = [sp.encode_as_ids(str(p)) for p in spec]\n",
    "\n",
    "# reduced step size from 3 to 1 because the model now learns by word pieces instead of chars\n",
    "# so stepping by 3 usually goes all the way to the next word\n",
    "step = 1\n",
    "window_size = 20\n",
    "\n",
    "spec = np.array(list(itertools.chain.from_iterable(spec)))\n",
    "print('len of data is', len(spec))\n",
    "print('Encoding...')\n",
    "\n",
    "X = np.array([spec[i:i+window_size] for i in range(0, len(spec)-window_size, step)])\n",
    "y = np.array([spec[i+window_size] for i in range(0, len(spec)-window_size, step)])\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase @ that thereby beautys rose might never die @ but as the riper should by time decease @ his tender heir might bear his memory @ but thou contracted to thine own bright eyes @ feedst thy lights flame with selfsubstantial fuel @ making a famine where abundance lies @ thy self thy foe to thy sweet self too cruel @ thou that art now the worlds fresh ornament @ and only herald to the gaudy spring @ within thine own bud buriest thy content @ and tender churl makst waste in niggarding @ pity the world or else this glutton be @ to eat the worlds due by the grave and thee @ \n",
      "['▁from', '▁fairest', '▁cre', 'ature', 's', '▁we', '▁desire', '▁inc', 'rea', 'se', '▁@', '▁that', '▁there', 'b', 'y', '▁beautys', '▁ro', 'se', '▁might', '▁never', '▁die', '▁@', '▁but', '▁as', '▁the', '▁ri', 'p', 'er', '▁should', '▁by', '▁time', '▁dec', 'ease', '▁@', '▁his', '▁tend', 'er', '▁he', 'ir', '▁might', '▁bear', '▁his', '▁mem', 'ory', '▁@', '▁but', '▁thou', '▁cont', 'ra', 'ct', 'ed', '▁to', '▁thine', '▁own', '▁bright', '▁eyes', '▁@', '▁fe', 'ed', 'st', '▁thy', '▁light', 's', '▁fl', 'ame', '▁with', '▁self', 'su', 'b', 'st', 'ant', 'ial', '▁f', 'u', 'el', '▁@', '▁making', '▁a', '▁fa', 'm', 'ine', '▁where', '▁ab', 'u', 'nd', 'ance', '▁lies', '▁@', '▁thy', '▁self', '▁thy', '▁fo', 'e', '▁to', '▁thy', '▁sweet', '▁self', '▁too', '▁cruel', '▁@', '▁thou', '▁that', '▁art', '▁now', '▁the', '▁worlds', '▁fresh', '▁or', 'na', 'ment', '▁@', '▁and', '▁only', '▁her', 'a', 'ld', '▁to', '▁the', '▁ga', 'u', 'dy', '▁spring', '▁@', '▁within', '▁thine', '▁own', '▁bu', 'd', '▁bu', 'ri', 'est', '▁thy', '▁cont', 'ent', '▁@', '▁and', '▁tend', 'er', '▁ch', 'ur', 'l', '▁mak', 'st', '▁w', 'aste', '▁in', '▁n', 'ig', 'g', 'ard', 'ing', '▁@', '▁pity', '▁the', '▁world', '▁or', '▁else', '▁this', '▁gl', 'ut', 't', 'on', '▁be', '▁@', '▁to', '▁ea', 't', '▁the', '▁worlds', '▁due', '▁by', '▁the', '▁gra', 've', '▁and', '▁thee', '▁@']\n",
      "len of data is (39355, 20) (39355,)\n",
      "Encoding...\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of window_size Sentence Pieces\n",
    "spec = np.array(processed_text.split('\\n'))\n",
    "\n",
    "print(spec[0])\n",
    "print(sp.encode_as_pieces(spec[0]))\n",
    "\n",
    "spec = [sp.encode_as_ids(str(p)) for p in spec]\n",
    "\n",
    "# reduced step size from 3 to 1 because the model now learns by word pieces instead of chars\n",
    "# so stepping by 3 usually goes all the way to the next word\n",
    "step = 1\n",
    "window_size = 20\n",
    "\n",
    "#spec = np.array(list(itertools.chain.from_iterable(spec)))\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# do it this way so training data does not cross poem boundaries\n",
    "for poem in spec:\n",
    "    for i in range(0, len(poem)-window_size, step):\n",
    "        X.append(poem[i:i+window_size])\n",
    "        y.append(poem[i+window_size])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "\n",
    "print('len of data is', X.shape, y.shape)\n",
    "print('Encoding...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def number_to_text(s):\n",
    "    return ''.join(sp.id_to_piece(int(i)) for i in s)\n",
    "\n",
    "def generate_seq(seed, n_pieces=50, temperature=1.0, use_sample=False):\n",
    "    \n",
    "    def reverse_formatting(s):\n",
    "        return s.replace('▁', ' ').replace('@', '\\n').replace('<n>', '')\n",
    "    \n",
    "    in_text = seed.copy()\n",
    "    result = ''\n",
    "    for _ in range(n_pieces):\n",
    "        yhat = model.predict(np.array([in_text[-window_size:]]), verbose=0)\n",
    "        yy = sample(yhat[0], temperature)\n",
    "        \n",
    "        if yy != 0:\n",
    "            in_text.append(yy)\n",
    "            result += (number_to_text([yy]))\n",
    "        \n",
    "    r_seed = reverse_formatting(number_to_text(seed))\n",
    "    print('seed: ' + r_seed)\n",
    "    print('temperature:', temperature)\n",
    "    print('generated:\\n' + r_seed + reverse_formatting(result), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 20, 256)           256000    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 1024)              3149824   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              1025000   \n",
      "=================================================================\n",
      "Total params: 4,430,824\n",
      "Trainable params: 4,430,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Bidirectional, LSTM, Dense, Dropout, Embedding\n",
    "#from keras.optimizers import Adam\n",
    "# tfa's LazyAdam is used because it is supposed to be more efficient for sparse\n",
    "# data. since we are using our own word vectors from the embedding layer this\n",
    "# should yield better results\n",
    "from tensorflow_addons.optimizers import LazyAdam\n",
    "from keras import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "units = 512\n",
    "\n",
    "input_ = Input(shape=(window_size,))\n",
    "\n",
    "emb = Embedding(vocab_size, units//2, input_length=window_size, \n",
    "                mask_zero=True, activity_regularizer=l2(1e-4))(input_)\n",
    "\n",
    "bidirectional = Bidirectional(LSTM(units, activation='relu', recurrent_dropout=0.1,\n",
    "                                   activity_regularizer=l2(1e-4)))(emb)\n",
    "\n",
    "predictor = Dense(vocab_size, activation='softmax', \n",
    "                  activity_regularizer=l2(1e-4))(bidirectional)\n",
    "\n",
    "model = Model(inputs=input_, outputs=predictor)\n",
    "optimizer = LazyAdam(clipnorm=1)\n",
    "model.compile(optimizer, 'sparse_categorical_crossentropy', \n",
    "              metrics=['sparse_categorical_crossentropy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "35840/39355 [==========================>...] - ETA: 7s - loss: 6.2489 - sparse_categorical_crossentropy: 6.1172"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "n_epochs = 250\n",
    "\n",
    "def on_epoch_end(epoch, _, epochs_split=10):\n",
    "    if epoch % epochs_split == 0 and epoch > 0:\n",
    "        \n",
    "        seed = sp.encode_as_ids(np.random.choice(poems))[:window_size]\n",
    "        \n",
    "        print(len(seed))\n",
    "        \n",
    "        print('generating poems:')\n",
    "        \n",
    "        for temp in [0.25, 0.75, 1.5]:\n",
    "            generate_seq(seed, temperature=temp, use_sample=True)\n",
    "            \n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "es_callback = EarlyStopping(monitor='loss', min_delta=0.01, patience=5, \n",
    "                            verbose=1, mode='auto')\n",
    "\n",
    "filepath=\"model-ckpt-epoch{epoch:08d}.hdf5\"\n",
    "checkpoint_callback = ModelCheckpoint(filepath, save_weights_only=True, period=10)\n",
    "\n",
    "model.fit(X, y, batch_size=256, epochs=n_epochs,\n",
    "          callbacks=[print_callback, es_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_poems(first_line, temps=[1.0]):\n",
    "    \n",
    "    def reverse_formatting(s):\n",
    "        return s.replace('▁', ' ').replace('@', '\\n').replace('<n>', '')\n",
    "    \n",
    "    seed = sp.encode_as_ids(first_line)    \n",
    "    print('seed: ' + reverse_formatting(first_line))\n",
    "\n",
    "  \n",
    "    for temperature in temps:\n",
    "        \n",
    "\n",
    "        result = ''\n",
    "        lines = 1\n",
    "        \n",
    "        in_text = seed.copy()\n",
    "        while len(in_text) < window_size:\n",
    "            in_text = [sp.pad_id()] + in_text\n",
    "            \n",
    "        while lines < 14 and len(result) < 1000:\n",
    "            yhat = model.predict(np.array([in_text[-window_size:]]), verbose=0)\n",
    "            yhat = sample(yhat[0], temperature)\n",
    "            \n",
    "            in_text.append(yhat)\n",
    "            result += (number_to_text([yhat]))\n",
    "\n",
    "            lines = result.count('@')\n",
    "\n",
    "        r_seed = reverse_formatting(number_to_text(seed))\n",
    "        print('temperature:', temperature)\n",
    "        print('generated:\\n' + r_seed + reverse_formatting(result), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: shall i compare thee to a summers day \n",
      "\n",
      "temperature: 0.25\n",
      "generated:\n",
      " shall i compare thee to a summers day \n",
      "again confound shall it given minute \n",
      "and that my sweet right day my headsdoing place \n",
      "with all pituent to like a sweet sweet sweet sweet sweet did desire \n",
      "but pride modd on her more than another that you \n",
      "which more milied when as i do appear \n",
      "then do not ye yet hard he that are are are have run \n",
      "how ie from loves was \n",
      "yet whoic one one whichther did see a baite \n",
      "which should from death do ye like perfoor  my sight \n",
      "by chotion wherewe it it so know \n",
      "whoeing hearts when me lose me such see \n",
      "what merit not not to tells when thoust \n",
      "and ekes by thy eyes to comvarvary \n",
      "and seeing wish is miled abobeeth with you and our eyes \n",
      " \n",
      "\n",
      "temperature: 0.75\n",
      "generated:\n",
      " shall i compare thee to a summers day \n",
      "again all feeds on and that doubt and righted \n",
      "for what it mercy will me when is let is read \n",
      "myso is long live a fools days \n",
      "for thy balw deck me ill or thine an esteem \n",
      "within do yet yet your greatre or rest \n",
      "al heart will a womans abotion where you abotion and you brave day \n",
      "withst shall be unto have seedtelling thee \n",
      "yet canif my faires if loves al manure me \n",
      "but from thy loves miscrameing \n",
      "and unto the sweet sweet mow dost steal thee treasure \n",
      "yet here envywhere all a love more \n",
      "odsuch hards that beauty is holy inate \n",
      "for thy self inffce of thy sweet love \n",
      "the hearts hard joy back more than her heart \n",
      " \n",
      "\n",
      "temperature: 1.5\n",
      "generated:\n",
      " shall i compare thee to a summers day \n",
      "then all confound knife shall bost and such \n",
      "inroughe creed of blessedd so eyes \n",
      "when let she knlessirspirea t \n",
      "for the thes of their in sweet face will a part \n",
      "but whom the worms more sweet gentle whereless doubt \n",
      "angy a cureedst me fortary torace \n",
      "oror bast losing one and see leave you a goldenen smce \n",
      "with fromuand thy child with myings do the hour idly \n",
      "to keep to otheronce light in to sacreededous \n",
      "to sweet senso framing my thought of sweet remor spend \n",
      "therefore shallthne wellth waste orfraishigh i myan but withh man untrly \n",
      "shall imagestfather me more one thou art remo \n",
      "and that for for mo thereaccusing to  thee \n",
      "not thee reien for ft \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"shall i compare thee to a summers day @\"\n",
    "write_poems(prompt, temps=[0.25, 0.75, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(from_seed, temp=0.5, fast=True, len_=100):\n",
    "    print(f'-- Generating Poem With Temperature: {temp}')\n",
    "\n",
    "    generated = []\n",
    "    sentence = sp.encode_as_pieces(from_seed)\n",
    "    generated += sentence\n",
    "    \n",
    "    print(len(sentence))\n",
    "    \n",
    "    print(generated)\n",
    "    \n",
    "    real_start = from_seed[len(from_seed) - len(from_seed.lstrip()):]\n",
    "    print(f'-- From seed: \\n\\\"{real_start}\\\"\\n')\n",
    "    \n",
    "    if not fast:\n",
    "        sys.stdout.write(real_start)\n",
    "\n",
    "    if not fast:\n",
    "        lines = 1\n",
    "        while lines < 14:\n",
    "            x_pred = np.zeros((1, window_size, vocab_size))\n",
    "            for t, wp in enumerate(sentence):\n",
    "                x_pred[0, t, sp.encode_as_ids(wp)] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temp)\n",
    "            next_wp = sp.id_to_piece(int(next_index))\n",
    "\n",
    "            sentence = sentence[1:] + [next_wp]\n",
    "\n",
    "            generated += [next_wp]\n",
    "            lines = generated.count('@')\n",
    "            print('generated', lines, 'lines')\n",
    "    else:\n",
    "        for _ in range(len_):\n",
    "            x_pred = np.zeros((1, window_size, vocab_size))\n",
    "            for t, wp in enumerate(sentence):\n",
    "                x_pred[0, t, sp.encode_as_ids(wp)] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, temp)\n",
    "            next_wp = sp.id_to_piece(int(next_index))\n",
    "\n",
    "            sentence = sentence[1:] + [next_wp]\n",
    "\n",
    "            generated += [next_wp]\n",
    "        \n",
    "    print(sp.decode_pieces(generated).replace('@', '\\n'), '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
